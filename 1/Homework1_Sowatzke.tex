\documentclass[fleqn]{article}
\usepackage[nodisplayskipstretch]{setspace}
\usepackage{amsmath, nccmath, bm}
\usepackage{amssymb}
\usepackage{enumitem}

\newcommand{\zerodisplayskip}{
	\setlength{\abovedisplayskip}{0pt}%
	\setlength{\belowdisplayskip}{0pt}%
	\setlength{\abovedisplayshortskip}{0pt}%
	\setlength{\belowdisplayshortskip}{0pt}%
	\setlength{\mathindent}{0pt}}
	
\title{Homework 1}
\author{Owen Sowatzke}
\date{February 7, 2024}

\begin{document}

	\offinterlineskip
	\setlength{\lineskip}{12pt}
	\zerodisplayskip
	\maketitle
	
	\begin{enumerate}
		\item Let $\mathbf{\Sigma_1}$ and $\mathbf{\Sigma_2}$ be two symmetric, positive definite matrices. Let $\mathbf{\Delta}$ be a diagonal matrix containing the eigenvalues of $\mathbf{\Sigma_1^{-1}}\mathbf{\Sigma_2}$. Let $\mathbf{A}$ contain the corresponding eigenvector of $\mathbf{\Sigma_1^{-1}}\mathbf{\Sigma_2}$ as its rows in the same order as the eigenvalue order in $\mathbf{\Sigma}$. Prove that
		
		\begin{equation*}
			\mathbf{A\Sigma_1A^T} = \mathbf{I}\quad\text{(Identity)}
		\end{equation*}
		
		\begin{equation*}
			\mathbf{A\Sigma_2A^T} = \mathbf{\Delta}\quad\text{(Diagonal)}
		\end{equation*}
		
		Because $\mathbf{\Sigma_1}$ is symmetric, it has orthogonal eigenvectors. Furthermore, because it is positive, its eigenvalues are all positive and nonzero. Let $\mathbf{D}$ be a diagonal matrix containing the eigenvalues $\mathbf{\Sigma_1}$, and let $\mathbf{B}$ contain the eigenvectors of $\mathbf{\Sigma_1}$ as its rows.
		
		\begin{equation*}
			\mathbf{B}\mathbf{\Sigma_1}\mathbf{B}^T = \mathbf{D} 
		\end{equation*}
		
		Because all the eigenvalues of $\mathbf{\Sigma_1}$ are positive and non-zero, we can invert $\mathbf{D}$ by inverting the entries on the main diagonal. Using $\mathbf{D}^{-1}$, we can express the identity matrix as follows:
		
		\begin{equation*}
			\mathbf{I} = \mathbf{D}\mathbf{D}^{-1}
		\end{equation*}
		
		Define $\mathbf{R}$ as the positive square root of $\mathbf{D}^{-1}$. $\mathbf{R}$ is a diagonal matrix obtained by taking the positive square root of the entries on the main diagonal of $\mathbf{D}^{-1}$. Using the operator $\mathbf{R}$, we can express the identity matrix as follows:
		
		\begin{equation*}
			\mathbf{I} = \mathbf{DRR}
		\end{equation*}
		
		Furthermore since diagonal matrices commute, we can rewrite the identity matrix as follows:
		
		\begin{equation*}
			\mathbf{I} = \mathbf{RDR} = \mathbf{R}\mathbf{D}\mathbf{R}^T
		\end{equation*}
		
		Multiplying the left side and right side of the eigenvalue-eigenvector equation above by $\mathbf{R}$ and $\mathbf{R}^T$ respectively, we can state the following:
		
		\begin{equation*}
			\mathbf{R}\mathbf{B}\mathbf{\Sigma_1}\mathbf{B}^T\mathbf{R}^T = (\mathbf{RB})\mathbf{\Sigma_1}(\mathbf{RB})^T = \mathbf{R}\mathbf{D}\mathbf{R}^T = \mathbf{I}
		\end{equation*}
		
		If we let $\mathbf{A} = \mathbf{RB}$, we have found an $\mathbf{A}$ which will produce the following result:
		
		\begin{equation*}
			\mathbf{A}\mathbf{\Sigma_1}\mathbf{A}^T = \mathbf{I}
		\end{equation*}
		
		Because $\mathbf{\Sigma_1}$ is symmetric and positive, we can found such $\mathbf{A}$ for all $\mathbf{\Sigma_1}$.
		
		The eigenvalue-eigenvector equation for $\mathbf{\Sigma_1}^{-1}\mathbf{\Sigma_2}$ can be written as follows:
		
		\begin{equation*}
			\mathbf{\Sigma_1}^{-1}\mathbf{\Sigma_2}\mathbf{A}^T = \mathbf{A}^T\mathbf{\Lambda}
		\end{equation*}
		
		Rearranging terms, we get the following expression:
		
		\begin{equation*}
			\mathbf{\Sigma_2}\mathbf{A}^T = \mathbf{\Sigma_1}\mathbf{A}^T\mathbf{\Lambda}
		\end{equation*}
		
		From above, we know there is a $\mathbf{A}$ such that $\mathbf{\Sigma_1}\mathbf{A}^T = \mathbf{A}^{-1}$. Substituting this value, we get the following result:
		 
		\begin{equation*}
			\mathbf{\Sigma_2}\mathbf{A}^T = \mathbf{A}^{-1}\mathbf{\Lambda}
		\end{equation*}
		
		Finally, after rearranging the equation, we end up with the following result:
		
		\begin{equation*}
			\mathbf{A}\mathbf{\Sigma_2}\mathbf{A}^T = \mathbf{\Lambda}
		\end{equation*}
		
		\item Consider two symmetric positive semi-definite matrices $\mathbf{\Sigma_1}$ and $\mathbf{\Sigma_2}$ of size $N \times N$. Let $\mathbf{P}$ be a $N \times N$ matrix such that
		
		\begin{equation*}
			\mathbf{P}(\mathbf{\Sigma_1} + \mathbf{\Sigma_2})\mathbf{P^T} = \mathbf{I}
		\end{equation*}
		
		If we define $\mathbf{A} = \mathbf{P}\mathbf{\Sigma_1}\mathbf{P^T}$ and $\mathbf{B} = \mathbf{P}\mathbf{\Sigma_2}\mathbf{P^T}$, then determine the properties of the eigenvalues of $\mathbf{A}$ and $\mathbf{B}$.
		
		$\mathbf{A}$ is symmetric positive semi-definite matrix if it can be written as $\mathbf{R}^T\mathbf{R}$.
		
		Because $\mathbf{\Sigma_1}$ is a symmetric positive semi-definite matrix, it has a positive square root $\mathbf{R_1}$ such that $\mathbf{\Sigma_1} = \mathbf{R_1}^T\mathbf{R_1}$.
		
		Therefore, we can rewrite $\mathbf{A}$ as follows:
		
		$\mathbf{A} = \mathbf{P}\mathbf{R_1}^T\mathbf{R_1}\mathbf{P}^T = (\mathbf{R_1}\mathbf{P}^T)^T(\mathbf{R_1}\mathbf{P}^T)$

		Define $\mathbf{R} = \mathbf{R_1}\mathbf{P}^T$. Now, $\mathbf{A}$ can be written as $\mathbf{R}^T\mathbf{R}$, so $\mathbf{A}$ is a symmetric positive semi-definite matrix.
		
		By similar logic, $\mathbf{B}$ is also a positive semi-definite matrix.
		
		Because $\mathbf{A}$ is a positive semi-definite matrix, it can be diagonalized using a matrix of orthonormal eigenvectors ($\mathbf{V}\mathbf{A}\mathbf{V}^T = \mathbf{\Lambda_A}$).
		
		If we diagonalize both sides of $\mathbf{A} = \mathbf{I} - \mathbf{B}$ using $\mathbf{V}$, we get:
		
		$\mathbf{V}\mathbf{A}\mathbf{V}^T = \mathbf{V}(\mathbf{I} - \mathbf{B})\mathbf{V}^T$
		
		$\mathbf{V}\mathbf{A}\mathbf{V}^T = \mathbf{V}\mathbf{I}\mathbf{V}^T - \mathbf{V}^T\mathbf{B}\mathbf{V}^T$
		
		$\mathbf{V}^T\mathbf{B}\mathbf{V}^T = \mathbf{V}\mathbf{I}\mathbf{V}^T - \mathbf{V}\mathbf{A}\mathbf{V}^T$
		
		$\mathbf{V}^T\mathbf{B}\mathbf{V}^T = \mathbf{V}\mathbf{V}^T - \mathbf{\Lambda_A}$
		
		Because the columns of $\mathbf{V}$ are orthonormal, $\mathbf{V}\mathbf{V}^T = \mathbf{I}$
		
		$\Rightarrow \mathbf{V}^T\mathbf{B}\mathbf{V}^T = \mathbf{I} - \mathbf{\Lambda_A}$
		
		$\mathbf{I} - \mathbf{\Lambda_A}$ is diagonal, so $\mathbf{B}$ can be diagonalized using the same matrix of orthonormal eigenvectors.
		
		$\mathbf{V}\mathbf{B}\mathbf{V}^T = \mathbf{\Lambda_B}$		
		
		$\mathbf{V}(\mathbf{A} + \mathbf{B})\mathbf{V}^T = \mathbf{V}\mathbf{I}\mathbf{V}^T$
		
		$\mathbf{\Lambda_A} + \mathbf{\Lambda_B} = \mathbf{I}$.
		
		Using the above expression, we can conclude that
		
		$\lambda_{A_i} + \lambda_{B_i} = 1\quad \forall\ i = 1,...,N$
		
		Furthermore, because $\mathbf{A}$ and $\mathbf{B}$ are positive semi-definite matrices we can conclude that
		
		$\lambda_{A_i} \geq 0$ and $\lambda_{B_i} \geq 0\quad \forall\ i = 1,...,N$
		
		\item Assume that $\mathbf{A}$ and $\mathbf{B}$ are two matrices of size $N \times N$, and that the inverse of $\mathbf{B}$ exists. Find the column vector $\mathbf{h}$ of length $N$ which maximizes the ratio
		
		\begin{equation*}
			J(\mathbf{h}) = \frac{\mathbf{h}^T\mathbf{A}\mathbf{h}}{\mathbf{h}^T\mathbf{B}\mathbf{h}}
		\end{equation*}
		
		We can find the value of $\mathbf{h}$, which maximizes the ratio $J(\mathbf{h})$, by solving for values of $\mathbf{h}$ which make $\nabla_J\mathbf{h} = \mathbf{0}$ 
		
		\begin{equation*}
			\nabla_J\mathbf{h} = \mathbf{0} \Rightarrow (\mathbf{A} + \mathbf{A}^T)\mathbf{h}\mathbf{h}^T\mathbf{B}\mathbf{h} - (\mathbf{B} + \mathbf{B}^T)\mathbf{h}\mathbf{h}^T\mathbf{A}\mathbf{h} = \mathbf{0}
		\end{equation*}
		
		\begin{equation*}
			\Rightarrow \mathbf{h}^T\mathbf{B}\mathbf{h}(\mathbf{A} + \mathbf{A}^T)\mathbf{h} = \mathbf{h}^T\mathbf{A}\mathbf{h}(\mathbf{B} + \mathbf{B}^T)\mathbf{h}
		\end{equation*}
		
		$\mathbf{h}^T\mathbf{A}\mathbf{h}$ and $\mathbf{h}^T\mathbf{B}\mathbf{h}$ are scalars, so we can group them into a single scalar term $\lambda$.
		
		\begin{equation*}
			\lambda = \frac{\mathbf{h}^T\mathbf{A}\mathbf{h}}{\mathbf{h}^T\mathbf{B}\mathbf{h}}
		\end{equation*}
		
		\begin{equation*}
			\Rightarrow (\mathbf{A} + \mathbf{A}^T)\mathbf{h} = \lambda(\mathbf{B} + \mathbf{B}^T)\mathbf{h}
		\end{equation*}
		
		\begin{equation*}
			\Rightarrow (\mathbf{B} + \mathbf{B}^T)^{-1}(\mathbf{A} + \mathbf{A}^T)\mathbf{h} = \lambda\mathbf{h} 
		\end{equation*}
		
		Note that the values of $\mathbf{h}$ which make $\nabla_\mathbf{h}J = \mathbf{0}$ are the eigenvalues of $(\mathbf{B} + \mathbf{B}^T)^{-1}(\mathbf{A} + \mathbf{A}^T)$
		
		Note that $\lambda$ is the value that we want to maximize, so the vector $\mathbf{h}$ which maximizes $\nabla_\mathbf{h}J = \mathbf{0}$ is the eigenvector of $(\mathbf{B} + \mathbf{B}^T)^{-1}(\mathbf{A} + \mathbf{A}^T)$ corresponding to the largest eigenvalue.
		
		If $\mathbf{A}$ and $\mathbf{B}$ are symmetric matrices, we find that $\mathbf{h}$ is the eigenvector of $\mathbf{B}^{-1}\mathbf{A}$ corresponding to the largest eigenvalue.
		
		\item Expand $(\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B})$ and $(\mathbf{A} - \mathbf{B})(\mathbf{A} + \mathbf{B})$. Are these expressions the same? If not, why not?
		
		\begin{equation*}
		(\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B}) = \mathbf{A^2} + \mathbf{B}\mathbf{A} - \mathbf{A}\mathbf{B} - \mathbf{B^2} = \mathbf{A^2} + (\mathbf{B}\mathbf{A} - \mathbf{A}\mathbf{B}) - \mathbf{B^2}
		\end{equation*}
		
		\begin{equation*}
		(\mathbf{A} - \mathbf{B})(\mathbf{A} + \mathbf{B}) = \mathbf{A^2} - \mathbf{B}\mathbf{A} + \mathbf{A}\mathbf{B} - \mathbf{B^2} = \mathbf{A^2} - (\mathbf{B}\mathbf{A} - \mathbf{A}\mathbf{B}) - \mathbf{B^2}
		\end{equation*}
		
		In general, matrix multiplication is not commutative.
		
		$\Rightarrow \mathbf{BA} \neq \mathbf{AB} \Rightarrow \mathbf{BA} - \mathbf{AB} \neq \mathbf{0}$
		
		$\therefore (\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B}) \neq (\mathbf{A} - \mathbf{B})(\mathbf{A} + \mathbf{B})$.
		
		So the above expressions are not the same because matrix multiplication is not always commutative.
		
		\item Let $\mathbf{x_i}$, $1 \leq i \leq N$ represent a set of column vectors, and $\mathbf{m} = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}}$ their average. Show that $\frac{1}{N}\sum_{i=1}^{N}\left[(\mathbf{x_i}-\mathbf{m})^T(\mathbf{x_i}-\mathbf{m})\right] = \left[\frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}\right]-\mathbf{m}^T\mathbf{m}$
		
		\begin{equation*}
			\frac{1}{N}\sum_{i=1}^{N}\left[(\mathbf{x_i}-\mathbf{m})^T(\mathbf{x_i}-\mathbf{m})\right] = \frac{1}{N}\sum_{i=1}^{N}\left[(\mathbf{x_i}^T-\mathbf{m}^T)(\mathbf{x_i}-\mathbf{m})\right]
		\end{equation*}
			
		\begin{equation*}
			 = \frac{1}{N}\sum_{i=1}^{N}\left[\mathbf{x_i}^T\mathbf{x_i}-\mathbf{m}^T\mathbf{x_i}-\mathbf{x_i}^T\mathbf{m}+\mathbf{m}^T\mathbf{m}\right]
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}-\frac{1}{N}\sum_{i=1}^{N}{\mathbf{m}^T\mathbf{x_i}}-\frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{m}}+\frac{1}{N}\sum_{i=1}^{N}{\mathbf{m}^T\mathbf{m}}
		\end{equation*}
				
		\begin{equation*}
			 = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}-\mathbf{m}^T\left(\frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}}\right)-\left(\frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}}\right)^T\mathbf{m}+\mathbf{m}^T\mathbf{m}
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}-\mathbf{m}^T\mathbf{m}-\mathbf{m}^T\mathbf{m}+\mathbf{m}^T\mathbf{m} = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}-\mathbf{m}^T\mathbf{m}
		\end{equation*}
		
		\item Assume equal probability for the birth of boys and girls. What is the probability that a four-child family chosen at random will have two boys and two girls, irrespective of the order of birth?
		
		The probability that a family of $n$ children has $k$ boys is given by a binomial distribution, where $p$ is the probability of that any given child is a boy.
		 
		\begin{equation*}
			P(X = k) = \binom{n}{k}p^k(1-p)^{n-k}
		\end{equation*}
		
		The probability that a four child family has 2 boys is then given by:
		
		\begin{equation*}
			P(X = 2) = \binom{4}{2}0.5^2(1-0.5)^{4-2} = \frac{4!}{2!(4-2)!}(0.5^4) = \frac{4!}{2!2!}\frac{1}{16} 
		\end{equation*}
		
		\begin{equation*}
			= \frac{4 \cdot 3}{2}\frac{1}{16} = \frac{6}{16} = \frac{3}{8}
		\end{equation*}
		
		\item The random variables $x$ and $y$ are distributed according to the bi-variate Gaussian PDF with the following parameters:
		
		$E\{x\} = \mu_x$,\quad$E\{y\} = \mu_y$,\quad$Var\{x\} = \sigma_x^2$,\quad Correlation coefficient $ = \rho$
		
		Determine the conditional PDF $p(y|x)$
		
		\begin{equation*}		
			p(y|x) = \frac{p(x,y)}{p(x)}
		\end{equation*}
		
		\begin{equation*}
			 = \frac{\frac{1}{2\pi\sigma_x\sigma_y\sqrt{1-\rho^2}}\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{(x-\mu_x)^2}{\sigma_x^2}-\frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y}+\frac{(y-\mu_y)^2}{\sigma_y^2}\right]\right\}}{\frac{1}{\sigma_x\sqrt{2\pi}}\text{exp}\left\{-\frac{(x-\mu_x)^2}{2\sigma_x^2}\right\}}
		\end{equation*}
		
		\begin{equation*}
			= \frac{1}{\sigma_y\sqrt{2\pi(1-\rho^2)}} \times
		\end{equation*}
	
		\begin{equation*}		
			\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{\rho^2(x-\mu_x)^2}{\sigma_x^2}-\frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y}+\frac{(y-\mu_y)^2}{\sigma_y^2}\right]\right\}
		\end{equation*}
	
		\begin{equation*}		
			= \frac{1}{\sigma_y\sqrt{2\pi(1-\rho^2)}}\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{\rho(x-\mu_x)}{\sigma_x} - \frac{y - \mu_y}{\sigma_y}\right]^2\right\}
		\end{equation*}
		
		\begin{equation*}		
			= \frac{1}{\sigma_y\sqrt{2\pi(1-\rho^2)}}\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{y - \mu_y}{\sigma_y} - \frac{\rho(x-\mu_x)}{\sigma_x}\right]^2\right\}
		\end{equation*}
		
		\begin{equation*}		
			= \frac{1}{\sigma_y\sqrt{2\pi(1-\rho^2)}}\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{(y - \mu_y) - \frac{\rho\sigma_y(x-\mu_x)}{\sigma_x}}{\sigma_y}\right]^2\right\}
		\end{equation*}
		
		\begin{equation*}		
			= \frac{1}{\sigma_y\sqrt{2\pi(1-\rho^2)}}\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{y - \left(\mu_y + \frac{\rho\sigma_y(x-\mu_x)}{\sigma_x}\right)}{\sigma_y}\right]^2\right\}
		\end{equation*}
		
		\begin{equation*}		
			= \frac{1}{\sigma_y\sqrt{2\pi(1-\rho^2)}}\text{exp}\left\{-\frac{1}{2\sigma_y^2(1-\rho^2)}\left[y - \left(\mu_y + \frac{\rho\sigma_y(x-\mu_x)}{\sigma_x}\right)\right]^2\right\}
		\end{equation*}
		
		\begin{equation*}		
			= \frac{1}{\sqrt{\sigma_y^2(1-\rho^2)}\sqrt{2\pi}}\text{exp}\left\{-\frac{1}{2\sigma_y^2(1-\rho^2)}\left[y - \left(\mu_y + \frac{\rho\sigma_y(x-\mu_x)}{\sigma_x}\right)\right]^2\right\}
		\end{equation*}
		
		\begin{equation*}
			p(y|x) = \mathcal{N}\left(\mu_y + \rho\frac{\sigma_y}{\sigma_x}(x-\mu_x),\:\sigma_y^2(1 - \rho^2)\right)
		\end{equation*}
		
		\item Here, the classifier has to choose between two classes "target present" and "target absent" by looking at two received values namely $r_1$ and $r_2$. Receive value $r_i$ is the sum of the transmitted value $s_i$ and noise in the channel $n_i$. Thus,
		
		$r_i = s_i + n_i$,\quad$i=1,2$
		
		The transmitter sends $[0 \quad 0]^T$ when "target is absent". In this problem, you have to tell the transmitter whether it should send $[1 \quad -1]^T$ or $ [1 \quad 1]^T$ as the vector for "target present". The objective is to minimize the probability of error. The noise samples $n_1$ and $n_2$ are bi-variate Gaussian with zero mean and unit variance. Assume that $P(\text{Target Present}) = 0.5$.
		
		\begin{enumerate}
			\item[(a)] Which of the two vectors $[1 \quad -1]^T$ or $[1 \quad 1]^T$ should denote "target present" if the two noise samples are uncorrelated? Explain why?
			
			The covariance matrix can be written as follows:
			
			\begin{equation*}
				\mathbf{\Sigma} = \begin{bmatrix}
					\sigma_{N_1}^2 & \text{Cov}(N_1,N_2) \\
					\text{Cov}(N_1,N_2) & \sigma_{N_2}^2
				\end{bmatrix}
			\end{equation*}
			
			The off-diagonal elements of the covariance matrix can be written in terms of the correlation coefficient as follows:
			 
			\begin{equation*}
				\rho = \frac{\text{Cov}(N_1,N_2)}{\sigma_{N_1}\sigma_{N_2}} \Rightarrow \text{Cov}(N_1,N_2) = \rho\sigma_{N_1}\sigma_{N_2} = \rho\sigma^2 = \rho
			\end{equation*}
			
			Substituting the above expression, the covariance matrix can be written as follows:
			
			\begin{equation*}
				\mathbf{\Sigma} = \begin{bmatrix}
					\sigma_{N_1}^2 & \rho \\
					\rho & \sigma_{N_2}^2
				\end{bmatrix} = \begin{bmatrix}
					\sigma^2 & \rho \\
					\rho & \sigma^2
				\end{bmatrix} = \begin{bmatrix}
					1 & \rho \\
					\rho & 1
				\end{bmatrix}
			\end{equation*}
			
			Because the noise samples from this part are uncorrelated $\rho = 0$ and
			
			\begin{equation*}
				\mathbf{\Sigma} = \begin{bmatrix}
					1 & 0 \\
					0 & 1
				\end{bmatrix} = \mathbf{I}
			\end{equation*}
			
			Let class 1 be "target present" and class 2 be "target is absent". For each set of transmitted values, we can compute the Mahalanobis distance between the two classes.
			
			For $[1 \quad 1]^T$:
			
			\begin{equation*}
				(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})
			\end{equation*}
			
			\begin{equation*}
				= \left(\begin{bmatrix} 1 \\ 1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)^TI^{-1}\left(\begin{bmatrix} 1 \\ 1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right) = \begin{bmatrix} 1 & 1 \end{bmatrix}\begin{bmatrix} 1 \\ 1 \end{bmatrix} = 2
			\end{equation*}
			
			\begin{equation*}
				\therefore d_M = \sqrt{(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})} = \sqrt{2}
			\end{equation*}
			
			For $[1 \quad -1]^T$:
			
			\begin{equation*}
				(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})
			\end{equation*}
			
			\begin{equation*}
				= \left(\begin{bmatrix} 1 \\ -1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)^TI^{-1}\left(\begin{bmatrix} 1 \\ -1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right) = \begin{bmatrix} 1 & -1 \end{bmatrix}\begin{bmatrix} 1 \\ -1 \end{bmatrix} = 2
			\end{equation*}
			
			\begin{equation*}
				\therefore d_M = \sqrt{(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})} = \sqrt{2}
			\end{equation*}
			
			Because the Mahanalobis distance between classes is the same for each set of transmitted values, the probability of error will also be the same. $\therefore$ Either vector can denote "target present" for this case.
			
			\item[(b)] Which of the two vectors $[1 \: -1]^T$ or $[1 \: 1]^T$ should denote "target present" if the two noise samples have a correlation coefficient of $\rho = 0.5$? Explain why?
			
			The covariance matrix for this case can be written as follows:
			
			\begin{equation*}
				\mathbf{\Sigma} = \begin{bmatrix}
					1 & \rho\\
					\rho & 1
				\end{bmatrix} = \begin{bmatrix}
					1 & 0.5\\
					0.5 & 1
				\end{bmatrix}
			\end{equation*}
			
			\begin{equation*}
				\mathbf{\Sigma}^{-1} = \frac{1}{1 - 0.5^2}\begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix}
			\end{equation*}
			
			\begin{equation*}
				 = \frac{1}{3/4}\begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix}
			\end{equation*}
			
			Let class 1 be "target present" and class 2 be "target is absent". For each set of transmitted values, we can compute the Mahalanobis distance between the two classes.
			
			For $[1 \quad 1]^T$:
			
			\begin{equation*}
				(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})
			\end{equation*}
			
			 \begin{equation*}
				= \left(\begin{bmatrix} 1 \\ 1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)^T\begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix}\left(\begin{bmatrix} 1 \\ 1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)
			\end{equation*}
			
			\begin{equation*}
				= \begin{bmatrix} 1 & 1 \end{bmatrix}\begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix}\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 1 \end{bmatrix}\begin{bmatrix} 2/3 \\ 2/3 \end{bmatrix} = 4/3
			\end{equation*}
			
			\begin{equation*}
				\therefore d_M = \sqrt{(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})} = 2/\sqrt{3}
			\end{equation*}
			
			For $[1 \quad -1]^T$:
			
			\begin{equation*}
				(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})
			\end{equation*}
			
			 \begin{equation*}
				= \left(\begin{bmatrix} 1 \\ -1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)^T\begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix}\left(\begin{bmatrix} 1 \\ -1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)
			\end{equation*}
			
			\begin{equation*}
				= \begin{bmatrix} 1 & -1 \end{bmatrix}\begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix}\begin{bmatrix} 1 \\ -1 \end{bmatrix} = \begin{bmatrix} 1 & -1 \end{bmatrix}\begin{bmatrix} 2 \\ -2 \end{bmatrix} = 4
			\end{equation*}
			
			\begin{equation*}
				\therefore d_M = \sqrt{(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})} = 2
			\end{equation*}
			
			Because the Mahanalobis distance between classes is greater when transmitting $[1 \quad -1]^T$, the probability of error will be smaller for this case. $\therefore [1 \quad -1]^T$ should denote "target present" for this case.
			
		\end{enumerate}
		
		\item Two variables $x_1$ and $x_2$ are observed in a two-class problem. A priori probability for class $\omega_1$ is 0.25; for class $\omega_1$, any combination of $x_1$ and $x_2$ such that $\left(x_1^2 + x_2^2\right) \leq 1$ can occur with equal probability. Similarly, for class $\omega_2$ all combinations of $x_1$ and $x_2$ such that $\left(x_1^2 + x_2^2\right) \leq 4$ occur with equal probability.
		
		\begin{enumerate}
			\item[(i)] determine the minimum probability of error in the above decision problem.
			
			The minimum $P_e$ classifier is
			
			$p(\omega_1|\mathbf{x}) \overset{\omega_1}{\underset{\omega_2}{\gtrless}} p(\omega_2|\mathbf{x})$
			
			$\Rightarrow p(\omega_1)p(\mathbf{x}|\omega_1) \overset{\omega_1}{\underset{\omega_2}{\gtrless}} p(\omega_2)p(\mathbf{x}|\omega_2)$
			
			$\Rightarrow 0.25p(\mathbf{x}|\omega_1) \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0.75p(\mathbf{x}|\omega_1)$
			
			$\Rightarrow p(\mathbf{x}|\omega_1) \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 3p(\mathbf{x}|\omega_1)$
			
			\begin{equation*}
				p(\mathbf{x}|\omega_1) = \begin{cases}
					\frac{1}{\pi} & x_1^2 + x_2^2 \leq 1 \\
					0 & \text{otherwise}
				\end{cases}
			\end{equation*}
			
			\begin{equation*}
				p(\mathbf{x}|\omega_2) = \begin{cases}
					\frac{1}{4\pi} & x_1^2 + x_2^2 \leq 4 \\
					0 & \text{otherwise}
				\end{cases}
			\end{equation*}
			
			Clearly, the decision threshold for this case should be $x_1^2 + x_2^2 = 1$ or $\mathbf{x}^T\mathbf{x} - 1 = 0$
			
			\begin{equation*}
				P_e = P(e|\omega_1)P(\omega_1) + P(e|\omega_2)P(\omega_2) = P(e|\omega_2)P(\omega_2)
			\end{equation*}
			
			\begin{equation*}
				= 0.75\int_{x_1=-1}^{1}{\int_{x_2=-\sqrt{1-x_2^2}}^{\sqrt{1-x_2^2}}{\left(\frac{1}{4\pi}\right)dx_2}dx_1}
			\end{equation*}
			
			\begin{equation*}
				= 0.75\int_{\theta=0}^{2\pi}{\int_{r=0}^{1}{\left(\frac{1}{4\pi}\right)rdr}d\theta} = 0.75\int_{\theta=0}^{2\pi}{\left.\left(\frac{1}{8\pi}\right)r^2\right\vert_{0}^{1}d\theta}
			\end{equation*}
			
			\begin{equation*}
				 = 0.75\int_{\theta=0}^{2\pi}{\left(\frac{1}{8\pi}\right)d\theta} = 0.75\left.\left(\frac{1}{8\pi}\right)\theta\right\vert_{0}^{2\pi} = 0.75\left(\frac{1}{4}\right) = \frac{3}{16}
			\end{equation*}
			
			\item[(ii)] Is the decision rule used in part (i) to obtain minimum $P_e$ a linear rule?
			
			No, the decision rule is quadratic in $\mathbf{x}$.
		\end{enumerate}
	\end{enumerate}
\end{document}