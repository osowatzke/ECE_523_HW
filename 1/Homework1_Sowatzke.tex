\documentclass[fleqn]{article}
\usepackage[nodisplayskipstretch]{setspace}
\usepackage{amsmath, nccmath, bm}
\usepackage{amssymb}
\usepackage{enumitem}

\newcommand{\zerodisplayskip}{
	\setlength{\abovedisplayskip}{0pt}%
	\setlength{\belowdisplayskip}{0pt}%
	\setlength{\abovedisplayshortskip}{0pt}%
	\setlength{\belowdisplayshortskip}{0pt}%
	\setlength{\mathindent}{0pt}}
	
\title{Homework 1}
\author{Owen Sowatzke}
\date{February 7, 2024}

\begin{document}

	\offinterlineskip
	\setlength{\lineskip}{12pt}
	\zerodisplayskip
	\maketitle
	
	\begin{enumerate}
		\item Let $\mathbf{\Sigma_1}$ and $\mathbf{\Sigma_2}$ be two symmetric, positive definite matrices. Let $\mathbf{\Delta}$ be a diagonal matrix containing the eigenvalues of $\mathbf{\Sigma_1^{-1}}\mathbf{\Sigma_2}$. Let $\mathbf{A}$ contain the corresponding eigenvector of $\mathbf{\Sigma_1^{-1}}\mathbf{\Sigma_2}$ as its rows in the same order as the eigenvalue order in $\mathbf{\Sigma}$. Prove that
		
		\begin{equation*}
			\mathbf{A\Sigma_1A^T} = \mathbf{I}\quad\text{(Identity)}
		\end{equation*}
		
		\begin{equation*}
			\mathbf{A\Sigma_2A^T} = \mathbf{\Delta}\quad\text{(Diagonal)}
		\end{equation*}
		
		Because $\mathbf{\Sigma_1}$ is symmetric, it has orthogonal eigenvectors. Furthermore, because it is positive, its eigenvalues are all positive and nonzero. Let $\mathbf{D}$ be a diagonal matrix containing the eigenvalues $\mathbf{\Sigma_1}$, and let $\mathbf{B}$ contain the eigenvectors of $\mathbf{\Sigma_1}$ as its rows.
		
		\begin{equation*}
			\mathbf{B}\mathbf{\Sigma_1}\mathbf{B}^T = \mathbf{D} 
		\end{equation*}
		
		Because all the eigenvalues of $\mathbf{\Sigma_1}$ are positive and non-zero, we can invert $\mathbf{D}$ by inverting the entries on the main diagonal. Using $\mathbf{D}^{-1}$, we can express the identity matrix as follows:
		
		\begin{equation*}
			\mathbf{I} = \mathbf{D}\mathbf{D}^{-1}
		\end{equation*}
		
		Define $\mathbf{R}$ as the positive square root of $\mathbf{D}^{-1}$. $\mathbf{R}$ is a diagonal matrix obtained by taking the positive square root of the entries on the main diagonal of $\mathbf{D}^{-1}$. Using the operator $\mathbf{R}$, we can express the identity matrix as follows:
		
		\begin{equation*}
			\mathbf{I} = \mathbf{DRR}
		\end{equation*}
		
		Furthermore since diagonal matrices commute, we can rewrite the identity matrix as follows:
		
		\begin{equation*}
			\mathbf{I} = \mathbf{RDR} = \mathbf{R}\mathbf{D}\mathbf{R}^T
		\end{equation*}
		
		Multiplying the left side and right side of the eigenvalue-eigenvector equation above by $\mathbf{R}$ and $\mathbf{R}^T$ respectively, we can state the following:
		
		\begin{equation*}
			\mathbf{R}\mathbf{B}\mathbf{\Sigma_1}\mathbf{B}^T\mathbf{R}^T = (\mathbf{RB})\mathbf{\Sigma_1}(\mathbf{RB})^T = \mathbf{R}\mathbf{D}\mathbf{R}^T = \mathbf{I}
		\end{equation*}
		
		If we let $\mathbf{A} = \mathbf{RB}$, we have found an $\mathbf{A}$ which will produce the following result:
		
		\begin{equation*}
			\mathbf{A}\mathbf{\Sigma_1}\mathbf{A}^T = \mathbf{I}
		\end{equation*}
		
		Because $\mathbf{\Sigma_1}$ is symmetric and positive, we can found such $\mathbf{A}$ for all $\mathbf{\Sigma_1}$.
		
		The eigenvalue-eigenvector equation for $\mathbf{\Sigma_1}^{-1}\mathbf{\Sigma_2}$ can be written as follows:
		
		\begin{equation*}
			\mathbf{\Sigma_1}^{-1}\mathbf{\Sigma_2}\mathbf{A}^T = \mathbf{A}^T\mathbf{\Lambda}
		\end{equation*}
		
		Rearranging terms, we get the following expression:
		
		\begin{equation*}
			\mathbf{\Sigma_2}\mathbf{A}^T = \mathbf{\Sigma_1}\mathbf{A}^T\mathbf{\Lambda}
		\end{equation*}
		
		From above, we know there is a $\mathbf{A}$ such that $\mathbf{\Sigma_1}\mathbf{A}^T = \mathbf{A}^{-1}$. Substituting this value, we get the following result:
		 
		\begin{equation*}
			\mathbf{\Sigma_2}\mathbf{A}^T = \mathbf{A}^{-1}\mathbf{\Lambda}
		\end{equation*}
		
		Finally, after rearranging the equation, we end up with the following result:
		
		\begin{equation*}
			\mathbf{A}\mathbf{\Sigma_2}\mathbf{A}^T = \mathbf{\Lambda}
		\end{equation*}
		
		\item Consider two symmetric positive semi-definite matrices $\mathbf{\Sigma_1}$ and $\mathbf{\Sigma_2}$ of size $N \times N$. Let $\mathbf{P}$ be a $N \times N$ matrix such that
		
		\begin{equation*}
			\mathbf{P}(\mathbf{\Sigma_1} + \mathbf{\Sigma_2})\mathbf{P^T} = \mathbf{I}
		\end{equation*}
		
		If we define $\mathbf{A} = \mathbf{P}\mathbf{\Sigma_1}\mathbf{P^T}$ and $\mathbf{B} = \mathbf{P}\mathbf{\Sigma_2}\mathbf{P^T}$, then determine the properties of the eigenvalues of $\mathbf{A}$ and $\mathbf{B}$.
		
		\item Assume that $\mathbf{A}$ and $\mathbf{B}$ are two matrices of size $N \times N$, and that the inverse of $\mathbf{B}$ exists. Find the column vector $\mathbf{h}$ of length $N$ which maximizes the ratio
		
		\begin{equation*}
			J(\mathbf{h}) = \frac{\mathbf{h}^T\mathbf{A}\mathbf{h}}{\mathbf{h}^T\mathbf{B}\mathbf{h}}
		\end{equation*}
		
		\begin{equation*}
			\mathbf{h}^T\mathbf{A}\mathbf{h} = \begin{bmatrix}h_1 & \cdots & h_N\end{bmatrix}\begin{bmatrix}a_{11} & \cdots & a_{1N} \\ \vdots & \ddots & \vdots \\ a_{N1} & \cdots & a_{NN}\end{bmatrix}\begin{bmatrix}h_1 \\ \vdots \\ h_N \end{bmatrix}
		\end{equation*}
		
		\begin{equation*}
			= \begin{bmatrix}h_1 & \cdots & h_N\end{bmatrix}\begin{bmatrix}a_{11}h_1 + \cdots + a_{1N}h_N \\ \vdots \\ a_{N1}h_1 + \cdots + a_{NN}h_N\end{bmatrix}
		\end{equation*}
		
		\begin{equation*}
			= h_1(a_{11}h_1 + \cdots + a_{1N}h_N) + \cdots + h_N( a_{N1}h_1 + \cdots + a_{NN}h_N)
		\end{equation*}
		
		\begin{equation*}
			\frac{\partial}{\partial{h_1}}\left(\mathbf{h}^T\mathbf{A}\mathbf{h}\right) = 2a_{11}h_1 + a_{12}h_2 + \cdots a_{1N}h_N + a_{21}h_2 + \cdots a_{N1}h_N
		\end{equation*}
		
		\begin{equation*}
			= a_{11}h_1 + \cdots a_{1N}h_N + a_{11}h_1 + \cdots a_{N1}h_N = \mathbf{a_{1.}h} + \mathbf{h^Ta_{.1}}
		\end{equation*}
		
		\begin{equation*}
			\frac{\partial{J(\mathbf{h})}}{\partial{h_1}} = \frac{(\mathbf{a_{1.}h} + \mathbf{h^Ta_{.1}})\mathbf{h}^T\mathbf{B}\mathbf{h} - \mathbf{h}^T\mathbf{A}\mathbf{h}(\mathbf{b_{1.}h} + \mathbf{h^Tb_{.1}})}{(\mathbf{h}^T\mathbf{B}\mathbf{h})^2}
		\end{equation*}
		
		\begin{equation*}
			\nabla_{\mathbf{h}}{J} = \frac{\partial{J(\mathbf{h})}}{\partial\mathbf{h}} = \begin{bmatrix}
				\frac{\partial{J(\mathbf{h})}}{\partial{h_1}} \\
				\vdots \\
				\frac{\partial{J(\mathbf{h})}}{\partial{h_N}}
			\end{bmatrix}
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{(\mathbf{h}^T\mathbf{B}\mathbf{h})^2}\begin{bmatrix}
				(\mathbf{a_{1.}h} + \mathbf{h^Ta_{.1}})\mathbf{h}^T\mathbf{B}\mathbf{h} - \mathbf{h}^T\mathbf{A}\mathbf{h}(\mathbf{b_{1.}h} + \mathbf{h^Tb_{.1}}) \\
				\vdots \\
				(\mathbf{a_{N.}h} + \mathbf{h}^T\mathbf{a_{.N}})\mathbf{h}^T\mathbf{B}\mathbf{h} - \mathbf{h}^T\mathbf{A}\mathbf{h}(\mathbf{b_{N.}h} + \mathbf{h}^T\mathbf{b_{.N}})
			\end{bmatrix}
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{(\mathbf{h}^T\mathbf{B}\mathbf{h})^2}\left(\left(\mathbf{Ah} + \left(\mathbf{h}^T\mathbf{A}\right)^T\right)\mathbf{h}^T\mathbf{B}\mathbf{h} - \left(\mathbf{Bh} + \left(\mathbf{h}^T\mathbf{B}\right)^T\right)\mathbf{h}^T\mathbf{A}\mathbf{h}\right)
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{(\mathbf{h}^T\mathbf{B}\mathbf{h})^2}\left(\left(\mathbf{Ah} + \mathbf{A}^T\mathbf{h}\right)\mathbf{h}^T\mathbf{B}\mathbf{h} - \left(\mathbf{Bh} + \mathbf{B}^T\mathbf{h}\right)\mathbf{h}^T\mathbf{A}\mathbf{h}\right)
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{(\mathbf{h}^T\mathbf{B}\mathbf{h})^2}\left(\left(\mathbf{A} + \mathbf{A}^T\right)\mathbf{h}\mathbf{h}^T\mathbf{B}\mathbf{h} - \left(\mathbf{B} + \mathbf{B}^T\right)\mathbf{h}\mathbf{h}^T\mathbf{A}\mathbf{h}\right)
		\end{equation*}
		
		\begin{equation*}
			\left(\mathbf{A} + \mathbf{A}^T\right)\mathbf{h}\mathbf{h}^T\mathbf{B}\mathbf{h} = \left(\mathbf{B} + \mathbf{B}^T\right)\mathbf{h}\mathbf{h}^T\mathbf{A}\mathbf{h}
		\end{equation*}
		
		\item Expand $(\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B})$ and $(\mathbf{A} - \mathbf{B})(\mathbf{A} + \mathbf{B})$. Are these expressions the same? If not, why not?
		
		\begin{equation*}
		(\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B}) = \mathbf{A^2} + \mathbf{B}\mathbf{A} - \mathbf{A}\mathbf{B} - \mathbf{B^2}
		\end{equation*}
		
		\begin{equation*}
		(\mathbf{A} - \mathbf{B})(\mathbf{A} + \mathbf{B}) = \mathbf{A^2} - \mathbf{B}\mathbf{A} + \mathbf{A}\mathbf{B} - \mathbf{B^2}
		\end{equation*}
		
		The above expressions are not the same because matrix multiplication is not always commutative ($\mathbf{AB} \neq \mathbf{BA}$).
		
		\item Let $\mathbf{x_i}$, $1 \leq i \leq N$ represent a set of column vectors, and $\mathbf{m} = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}}$ their average. Show that $\frac{1}{N}\sum_{i=1}^{N}\left[(\mathbf{x_i}-\mathbf{m})^T(\mathbf{x_i}-\mathbf{m})\right] = \left[\frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}\right]-\mathbf{m}^T\mathbf{m}$
		
		\begin{equation*}
			\frac{1}{N}\sum_{i=1}^{N}\left[(\mathbf{x_i}-\mathbf{m})^T(\mathbf{x_i}-\mathbf{m})\right] = \frac{1}{N}\sum_{i=1}^{N}\left[(\mathbf{x_i}^T-\mathbf{m}^T)(\mathbf{x_i}-\mathbf{m})\right]
		\end{equation*}
			
		\begin{equation*}
			 = \frac{1}{N}\sum_{i=1}^{N}\left[\mathbf{x_i}^T\mathbf{x_i}-\mathbf{m}^T\mathbf{x_i}-\mathbf{x_i}^T\mathbf{m}+\mathbf{m}^T\mathbf{m}\right]
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}-\frac{1}{N}\sum_{i=1}^{N}{\mathbf{m}^T\mathbf{x_i}}-\frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{m}}+\frac{1}{N}\sum_{i=1}^{N}{\mathbf{m}^T\mathbf{m}}
		\end{equation*}
				
		\begin{equation*}
			 = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}-\mathbf{m}^T\left(\frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}}\right)-\left(\frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}}\right)^T\mathbf{m}+\mathbf{m}^T\mathbf{m}
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}-\mathbf{m}^T\mathbf{m}-\mathbf{m}^T\mathbf{m}+\mathbf{m}^T\mathbf{m} = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}-\mathbf{m}^T\mathbf{m}
		\end{equation*}
		
		\item Assume equal probability for the birth of boys and girls. What is the probability that a four-child family chosen at random will have two boys and two girls, irrespective of the order of birth?
		
		\begin{equation*}
			P = \binom{n}{k}p^k(1-p)^{n-k} = \binom{4}{2}0.5^2(1-0.5)^{4-2} = \frac{4!}{2!(4-2)!}(0.5^4)
		\end{equation*}
		
		\begin{equation*}
			 = \frac{4!}{2!2!}\frac{1}{16} = \frac{4 \cdot 3}{2}\frac{1}{16} = \frac{6}{16} = \frac{3}{8}
		\end{equation*}
		
		\item The random variables $x$ and $y$ are distributed according to the bi-variate Gaussian PDF with the following parameters:
		
		$E\{x\} = \mu_x$,\quad$E\{y\} = \mu_y$,\quad$Var\{x\} = \sigma_x^2$,\quad Correlation coefficient $ = \rho$
		
		Determine the conditional PDF $p(y|x)$
		
		\begin{equation*}		
			p(y|x) = \frac{p(x,y)}{p(x)}
		\end{equation*}
		
		\begin{equation*}
			 = \frac{\frac{1}{2\pi\sigma_x\sigma_y\sqrt{1-\rho^2}}\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{(x-\mu_x)^2}{\sigma_x^2}-\frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y}+\frac{(y-\mu_y)^2}{\sigma_y^2}\right]\right\}}{\frac{1}{\sigma_x\sqrt{2\pi}}\text{exp}\left\{-\frac{(x-\mu_x)^2}{2\sigma_x^2}\right\}}
		\end{equation*}
		
		\begin{equation*}
			= \frac{1}{\sigma_y\sqrt{2\pi(1-\rho^2)}} \times
		\end{equation*}
	
		\begin{equation*}		
			\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{\rho^2(x-\mu_x)^2}{\sigma_x^2}-\frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y}+\frac{(y-\mu_y)^2}{\sigma_y^2}\right]\right\}
		\end{equation*}
	
		\begin{equation*}		
			= \frac{1}{\sigma_y\sqrt{2\pi(1-\rho^2)}}\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{\rho(x-\mu_x)}{\sigma_x} - \frac{y - \mu_y}{\sigma_y}\right]^2\right\}
		\end{equation*}
		
		\begin{equation*}		
			= \frac{1}{\sigma_y\sqrt{2\pi(1-\rho^2)}}\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{y - \mu_y}{\sigma_y} - \frac{\rho(x-\mu_x)}{\sigma_x}\right]^2\right\}
		\end{equation*}
		
		\begin{equation*}		
			= \frac{1}{\sigma_y\sqrt{2\pi(1-\rho^2)}}\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{(y - \mu_y) - \frac{\rho\sigma_y(x-\mu_x)}{\sigma_x}}{\sigma_y}\right]^2\right\}
		\end{equation*}
		
		\begin{equation*}		
			= \frac{1}{\sigma_y\sqrt{2\pi(1-\rho^2)}}\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{y - \left(\mu_y + \frac{\rho\sigma_y(x-\mu_x)}{\sigma_x}\right)}{\sigma_y}\right]^2\right\}
		\end{equation*}
		
		\begin{equation*}		
			= \frac{1}{\sigma_y\sqrt{2\pi(1-\rho^2)}}\text{exp}\left\{-\frac{1}{2\sigma_y^2(1-\rho^2)}\left[y - \left(\mu_y + \frac{\rho\sigma_y(x-\mu_x)}{\sigma_x}\right)\right]^2\right\}
		\end{equation*}
		
		\begin{equation*}		
			= \frac{1}{\sqrt{\sigma_y^2(1-\rho^2)}\sqrt{2\pi}}\text{exp}\left\{-\frac{1}{2\sigma_y^2(1-\rho^2)}\left[y - \left(\mu_y + \frac{\rho\sigma_y(x-\mu_x)}{\sigma_x}\right)\right]^2\right\}
		\end{equation*}
		
		\begin{equation*}
			p(y|x) = \mathcal{N}\left(\mu_y + \rho\frac{\sigma_y}{\sigma_x}(x-\mu_x),\:\sigma_y^2(1 - \rho^2)\right)
		\end{equation*}
		
		\item Here, the classifier has to choose between two classes "target present" and "target absent" by looking at two received values namely $r_1$ and $r_2$. Receive value $r_i$ is the sum of the transmitted value $s_i$ and noise in the channel $n_i$. Thus,
		
		$r_i = s_i + n_i$,\quad$i=1,2$
		
		The transmitter sends $[0 \quad 0]^T$ when "target is absent". In this problem, you have to tell the transmitter whether it should send $[1 \quad -1]^T$ or $ [1 \quad 1]^T$ as the vector for "target present". The objective is to minimize the probability of error. The noise samples $n_1$ and $n_2$ are bi-variate Gaussian with zero mean and unit variance. Assume that $P(\text{Target Present}) = 0.5$.
		
		\begin{enumerate}
			\item[(a)] Which of the two vectors $[1 \quad -1]^T$ or $[1 \quad 1]^T$ should denote "target present" if the two noise samples are uncorrelated? Explain why?
			
			The covariance matrix can be written as follows:
			
			\begin{equation*}
				\mathbf{\Sigma} = \begin{bmatrix}
					\sigma_{N_1}^2 & \text{Cov}(N_1,N_2) \\
					\text{Cov}(N_1,N_2) & \sigma_{N_2}^2
				\end{bmatrix}
			\end{equation*}
			
			The off-diagonal elements of the covariance matrix can be written in terms of the correlation coefficient as follows:
			 
			\begin{equation*}
				\rho = \frac{\text{Cov}(N_1,N_2)}{\sigma_{N_1}\sigma_{N_2}} \Rightarrow \text{Cov}(N_1,N_2) = \rho\sigma_{N_1}\sigma_{N_2} = \rho\sigma^2 = \rho
			\end{equation*}
			
			Substituting the above expression, the covariance matrix can be written as follows:
			
			\begin{equation*}
				\mathbf{\Sigma} = \begin{bmatrix}
					\sigma_{N_1}^2 & \rho \\
					\rho & \sigma_{N_2}^2
				\end{bmatrix} = \begin{bmatrix}
					\sigma^2 & \rho \\
					\rho & \sigma^2
				\end{bmatrix} = \begin{bmatrix}
					1 & \rho \\
					\rho & 1
				\end{bmatrix}
			\end{equation*}
			
			Because the noise samples from this part are uncorrelated $\rho = 0$ and
			
			\begin{equation*}
				\mathbf{\Sigma} = \begin{bmatrix}
					1 & 0 \\
					0 & 1
				\end{bmatrix} = \mathbf{I}
			\end{equation*}
			
			Let class 1 be "target present" and class 2 be "target is absent". For each set of transmitted values, we can compute the Mahalanobis distance between the two classes.
			
			For $[1 \quad 1]^T$:
			
			\begin{equation*}
				(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})
			\end{equation*}
			
			\begin{equation*}
				= \left(\begin{bmatrix} 1 \\ 1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)^TI^{-1}\left(\begin{bmatrix} 1 \\ 1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right) = \begin{bmatrix} 1 & 1 \end{bmatrix}\begin{bmatrix} 1 \\ 1 \end{bmatrix} = 2
			\end{equation*}
			
			\begin{equation*}
				\therefore d_M = \sqrt{(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})} = \sqrt{2}
			\end{equation*}
			
			For $[1 \quad -1]^T$:
			
			\begin{equation*}
				(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})
			\end{equation*}
			
			\begin{equation*}
				= \left(\begin{bmatrix} 1 \\ -1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)^TI^{-1}\left(\begin{bmatrix} 1 \\ -1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right) = \begin{bmatrix} 1 & -1 \end{bmatrix}\begin{bmatrix} 1 \\ -1 \end{bmatrix} = 2
			\end{equation*}
			
			\begin{equation*}
				\therefore d_M = \sqrt{(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})} = \sqrt{2}
			\end{equation*}
			
			Because the Mahanalobis distance between classes is the same for each set of transmitted values, the probability of error will also be the same. $\therefore$ Either vector can denote "target present" for this case.
			
			\item[(b)] Which of the two vectors $[1 \: -1]^T$ or $[1 \: 1]^T$ should denote "target present" if the two noise samples have a correlation coefficient of $\rho = 0.5$? Explain why?
			
			The covariance matrix for this case can be written as follows:
			
			\begin{equation*}
				\mathbf{\Sigma} = \begin{bmatrix}
					1 & \rho\\
					\rho & 1
				\end{bmatrix} = \begin{bmatrix}
					1 & 0.5\\
					0.5 & 1
				\end{bmatrix}
			\end{equation*}
			
			\begin{equation*}
				\mathbf{\Sigma}^{-1} = \frac{1}{1 - 0.5^2}\begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix}
			\end{equation*}
			
			\begin{equation*}
				 = \frac{1}{3/4}\begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix} = \begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix}
			\end{equation*}
			
			Let class 1 be "target present" and class 2 be "target is absent". For each set of transmitted values, we can compute the Mahalanobis distance between the two classes.
			
			For $[1 \quad 1]^T$:
			
			\begin{equation*}
				(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})
			\end{equation*}
			
			 \begin{equation*}
				= \left(\begin{bmatrix} 1 \\ 1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)^T\begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix}\left(\begin{bmatrix} 1 \\ 1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)
			\end{equation*}
			
			\begin{equation*}
				= \begin{bmatrix} 1 & 1 \end{bmatrix}\begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix}\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 1 \end{bmatrix}\begin{bmatrix} 2/3 \\ 2/3 \end{bmatrix} = 4/3
			\end{equation*}
			
			\begin{equation*}
				\therefore d_M = \sqrt{(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})} = 2/\sqrt{3}
			\end{equation*}
			
			For $[1 \quad -1]^T$:
			
			\begin{equation*}
				(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})
			\end{equation*}
			
			 \begin{equation*}
				= \left(\begin{bmatrix} 1 \\ -1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)^T\begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix}\left(\begin{bmatrix} 1 \\ -1 \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \end{bmatrix}\right)
			\end{equation*}
			
			\begin{equation*}
				= \begin{bmatrix} 1 & -1 \end{bmatrix}\begin{bmatrix} 4/3 & -2/3 \\ -2/3 & 4/3 \end{bmatrix}\begin{bmatrix} 1 \\ -1 \end{bmatrix} = \begin{bmatrix} 1 & -1 \end{bmatrix}\begin{bmatrix} 2 \\ -2 \end{bmatrix} = 4
			\end{equation*}
			
			\begin{equation*}
				\therefore d_M = \sqrt{(\mathbf{\mu_1} - \mathbf{\mu_2})^T\mathbf{\Sigma}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_2})} = 2
			\end{equation*}
			
			Because the Mahanalobis distance between classes is greater when transmitting $[1 \quad -1]^T$, the probability of error will be smaller for this case. $\therefore [1 \quad -1]^T$ should denote "target present" for this case.
			
		\end{enumerate}
	\end{enumerate}
\end{document}