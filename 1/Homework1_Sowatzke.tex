\documentclass[fleqn]{article}
\usepackage[nodisplayskipstretch]{setspace}
\usepackage{amsmath, nccmath, bm}
\usepackage{amssymb}
\usepackage{enumitem}

\newcommand{\zerodisplayskip}{
	\setlength{\abovedisplayskip}{0pt}%
	\setlength{\belowdisplayskip}{0pt}%
	\setlength{\abovedisplayshortskip}{0pt}%
	\setlength{\belowdisplayshortskip}{0pt}%
	\setlength{\mathindent}{0pt}}
	
\title{Homework 1}
\author{Owen Sowatzke}
\date{February 7, 2024}

\begin{document}

	\offinterlineskip
	\setlength{\lineskip}{12pt}
	\zerodisplayskip
	\maketitle
	
	\begin{enumerate}
		\item Let $\mathbf{\Sigma_1}$ and $\mathbf{\Sigma_2}$ be two symmetric, positive definite matrices. Let $\mathbf{\Delta}$ be a diagonal matrix containing the eigenvalues of $\mathbf{\Sigma_1^{-1}}\mathbf{\Sigma_2}$. Let $\mathbf{A}$ contain the corresponding eigenvector of $\mathbf{\Sigma_1^{-1}}\mathbf{\Sigma_2}$ as its rows in the same order as the eigenvalue order in $\mathbf{\Sigma}$. Prove that
		
		\begin{equation*}
			\mathbf{A\Sigma_1A^T} = \mathbf{I}\quad\text{(Identity)}
		\end{equation*}
		
		\begin{equation*}
			\mathbf{A\Sigma_2A^T} = \mathbf{\Delta}\quad\text{(Diagonal)}
		\end{equation*}
		
		\begin{equation*}
			\mathbf{A\Sigma_1^{-1}\Sigma_2A^T = \Delta}
		\end{equation*}
		
		\begin{equation*}
			\mathbf{\Sigma_2A^T = \Sigma_1A^T\Delta}
		\end{equation*}
		
		\item Consider two symmetric positive semi-definite matrices $\mathbf{\Sigma_1}$ and $\mathbf{\Sigma_2}$ of size $N \times N$. Let $\mathbf{P}$ be a $N \times N$ matrix such that
		
		\begin{equation*}
			\mathbf{P}(\mathbf{\Sigma_1} + \mathbf{\Sigma_2})\mathbf{P^T} = \mathbf{I}
		\end{equation*}
		
		If we define $\mathbf{A} = \mathbf{P}\mathbf{\Sigma_1}\mathbf{P^T}$ and $\mathbf{B} = \mathbf{P}\mathbf{\Sigma_2}\mathbf{P^T}$, then determine the properties of the eigenvalues of $\mathbf{A}$ and $\mathbf{B}$.
		
		\item Assume that $\mathbf{A}$ and $\mathbf{B}$ are two matrices of size $N \times N$, and that the inverse of $\mathbf{B}$ exists. Find the column vector $\mathbf{h}$ of length $N$ which maximizes the ratio
		
		\begin{equation*}
			J(\mathbf{h}) = \frac{\mathbf{h}^T\mathbf{A}\mathbf{h}}{\mathbf{h}^T\mathbf{B}\mathbf{h}}
		\end{equation*}
		
		\begin{equation*}
			\mathbf{h}^T\mathbf{A}\mathbf{h} = \begin{bmatrix}h_1 & \cdots & h_N\end{bmatrix}\begin{bmatrix}a_{11} & \cdots & a_{1N} \\ \vdots & \ddots & \vdots \\ a_{N1} & \cdots & a_{NN}\end{bmatrix}\begin{bmatrix}h_1 \\ \vdots \\ h_N \end{bmatrix}
		\end{equation*}
		
		\begin{equation*}
			= \begin{bmatrix}h_1 & \cdots & h_N\end{bmatrix}\begin{bmatrix}a_{11}h_1 + \cdots + a_{1N}h_N \\ \vdots \\ a_{N1}h_1 + \cdots + a_{NN}h_N\end{bmatrix}
		\end{equation*}
		
		\begin{equation*}
			= h_1(a_{11}h_1 + \cdots + a_{1N}h_N) + \cdots + h_N( a_{N1}h_1 + \cdots + a_{NN}h_N)
		\end{equation*}
		
		\begin{equation*}
			\frac{\partial}{\partial{h_1}}\left(\mathbf{h}^T\mathbf{A}\mathbf{h}\right) = 2a_{11}h_1 + a_{12}h_2 + \cdots a_{1N}h_N + a_{21}h_2 + \cdots a_{N1}h_N
		\end{equation*}
		
		\begin{equation*}
			= a_{11}h_1 + \cdots a_{1N}h_N + a_{11}h_1 + \cdots a_{N1}h_N = \mathbf{a_{1.}h} + \mathbf{h^Ta_{.1}}
		\end{equation*}
		
		\begin{equation*}
			\frac{\partial{J(\mathbf{h})}}{\partial{h_1}} = \frac{(\mathbf{a_{1.}h} + \mathbf{h^Ta_{.1}})\mathbf{h}^T\mathbf{B}\mathbf{h} - \mathbf{h}^T\mathbf{A}\mathbf{h}(\mathbf{b_{1.}h} + \mathbf{h^Tb_{.1}})}{(\mathbf{h}^T\mathbf{B}\mathbf{h})^2}
		\end{equation*}
		
		\begin{equation*}
			\nabla_{\mathbf{h}}{J} = \frac{\partial{J(\mathbf{h})}}{\partial\mathbf{h}} = \begin{bmatrix}
				\frac{\partial{J(\mathbf{h})}}{\partial{h_1}} \\
				\vdots \\
				\frac{\partial{J(\mathbf{h})}}{\partial{h_N}}
			\end{bmatrix}
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{(\mathbf{h}^T\mathbf{B}\mathbf{h})^2}\begin{bmatrix}
				(\mathbf{a_{1.}h} + \mathbf{h^Ta_{.1}})\mathbf{h}^T\mathbf{B}\mathbf{h} - \mathbf{h}^T\mathbf{A}\mathbf{h}(\mathbf{b_{1.}h} + \mathbf{h^Tb_{.1}}) \\
				\vdots \\
				(\mathbf{a_{N.}h} + \mathbf{h}^T\mathbf{a_{.N}})\mathbf{h}^T\mathbf{B}\mathbf{h} - \mathbf{h}^T\mathbf{A}\mathbf{h}(\mathbf{b_{N.}h} + \mathbf{h}^T\mathbf{b_{.N}})
			\end{bmatrix}
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{(\mathbf{h}^T\mathbf{B}\mathbf{h})^2}\left(\left(\mathbf{Ah} + \left(\mathbf{h}^T\mathbf{A}\right)^T\right)\mathbf{h}^T\mathbf{B}\mathbf{h} - \left(\mathbf{Bh} + \left(\mathbf{h}^T\mathbf{B}\right)^T\right)\mathbf{h}^T\mathbf{A}\mathbf{h}\right)
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{(\mathbf{h}^T\mathbf{B}\mathbf{h})^2}\left(\left(\mathbf{Ah} + \mathbf{A}^T\mathbf{h}\right)\mathbf{h}^T\mathbf{B}\mathbf{h} - \left(\mathbf{Bh} + \mathbf{B}^T\mathbf{h}\right)\mathbf{h}^T\mathbf{A}\mathbf{h}\right)
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{(\mathbf{h}^T\mathbf{B}\mathbf{h})^2}\left(\left(\mathbf{A} + \mathbf{A}^T\right)\mathbf{h}\mathbf{h}^T\mathbf{B}\mathbf{h} - \left(\mathbf{B} + \mathbf{B}^T\right)\mathbf{h}\mathbf{h}^T\mathbf{A}\mathbf{h}\right)
		\end{equation*}
		
		\begin{equation*}
			\left(\mathbf{A} + \mathbf{A}^T\right)\mathbf{h}\mathbf{h}^T\mathbf{B}\mathbf{h} = \left(\mathbf{B} + \mathbf{B}^T\right)\mathbf{h}\mathbf{h}^T\mathbf{A}\mathbf{h}
		\end{equation*}
		
		\item Expand $(\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B})$ and $(\mathbf{A} - \mathbf{B})(\mathbf{A} + \mathbf{B})$. Are these expressions the same? If not, why not?
		
		\begin{equation*}
		(\mathbf{A} + \mathbf{B})(\mathbf{A} - \mathbf{B}) = \mathbf{A^2} + \mathbf{B}\mathbf{A} - \mathbf{A}\mathbf{B} - \mathbf{B^2}
		\end{equation*}
		
		\begin{equation*}
		(\mathbf{A} - \mathbf{B})(\mathbf{A} + \mathbf{B}) = \mathbf{A^2} - \mathbf{B}\mathbf{A} + \mathbf{A}\mathbf{B} - \mathbf{B^2}
		\end{equation*}
		
		The above expressions are not the same because matrix multiplication is not always commutative ($\mathbf{AB} \neq \mathbf{BA}$).
		
		\item Let $\mathbf{x_i}$, $1 \leq i \leq N$ represent a set of column vectors, and $\mathbf{m} = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}}$ their average. Show that $\frac{1}{N}\sum_{i=1}^{N}\left[(\mathbf{x_i}-\mathbf{m})^T(\mathbf{x_i}-\mathbf{m})\right] = \left[\frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}\right]-\mathbf{m}^T\mathbf{m}$
		
		\begin{equation*}
			\frac{1}{N}\sum_{i=1}^{N}\left[(\mathbf{x_i}-\mathbf{m})^T(\mathbf{x_i}-\mathbf{m})\right] = \frac{1}{N}\sum_{i=1}^{N}\left[(\mathbf{x_i}^T-\mathbf{m}^T)(\mathbf{x_i}-\mathbf{m})\right]
		\end{equation*}
			
		\begin{equation*}
			 = \frac{1}{N}\sum_{i=1}^{N}\left[\mathbf{x_i}^T\mathbf{x_i}-\mathbf{m}^T\mathbf{x_i}-\mathbf{x_i}^T\mathbf{m}+\mathbf{m}^T\mathbf{m}\right]
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}-\frac{1}{N}\sum_{i=1}^{N}{\mathbf{m}^T\mathbf{x_i}}-\frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{m}}+\frac{1}{N}\sum_{i=1}^{N}{\mathbf{m}^T\mathbf{m}}
		\end{equation*}
				
		\begin{equation*}
			 = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}-\mathbf{m}^T\left(\frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}}\right)-\left(\frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}}\right)^T\mathbf{m}+\mathbf{m}^T\mathbf{m}
		\end{equation*}
		
		\begin{equation*}
			 = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}-\mathbf{m}^T\mathbf{m}-\mathbf{m}^T\mathbf{m}+\mathbf{m}^T\mathbf{m} = \frac{1}{N}\sum_{i=1}^{N}{\mathbf{x_i}^T\mathbf{x_i}}-\mathbf{m}^T\mathbf{m}
		\end{equation*}
		
		\item Assume equal probability for the birth of boys and girls. What is the probability that a four-child family chosen at random will have two boys and two girls, irrespective of the order of birth?
		
		\begin{equation*}
			P = \binom{n}{k}p^k(1-p)^{n-k} = \binom{4}{2}0.5^2(1-0.5)^{4-2} = \frac{4!}{2!(4-2)!}(0.5^4)
		\end{equation*}
		
		\begin{equation*}
			 = \frac{4!}{2!2!}\frac{1}{16} = \frac{4 \cdot 3}{2}\frac{1}{16} = \frac{6}{16} = \frac{3}{8}
		\end{equation*}
		
		\item The random variables $x$ and $y$ are distributed according to the bi-variate Gaussian PDF with the following parameters:
		
		$E\{x\} = \mu_x$,\quad$E\{y\} = \mu_y$,\quad$Var\{x\} = \sigma_x^2$,\quad Correlation coefficient $ = \rho$
		
		Determine the conditional PDF $p(y|x)$
		
		\begin{equation*}		
			p(y|x) = \frac{p(x,y)}{p(y)}
		\end{equation*}
		
		\begin{equation*}
			 = \frac{\frac{1}{2\pi\sigma_x\sigma_y\sqrt{1-\rho^2}}\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{(x-\mu_x)^2}{\sigma_x^2}-\frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y}+\frac{(y-\mu_y)^2}{\sigma_y^2}\right]\right\}}{\frac{1}{\sigma_y\sqrt{2\pi}}\text{exp}\left\{-\frac{(y-\mu_y)^2}{2\sigma_y^2}\right\}}
		\end{equation*}
		
		\begin{equation*}
			= \frac{1}{\sigma_x\sqrt{2\pi(1-\rho^2)}} \times
		\end{equation*}
	
		\begin{equation*}		
			\text{exp}\left\{-\frac{1}{2(1-\rho^2)}\left[\frac{(x-\mu_x)^2}{\sigma_x^2}-\frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y}+\frac{\rho^2(y-\mu_y)^2}{\sigma_y^2}\right]\right\}
		\end{equation*}
		
		\item Here, the classifier has to choose between two classes "target present" and "target absent" by looking at two received values namely $r_1$ and $r_2$. Receive value $r_i$ is the sume of the transmitted value $s_i$ and noise in the channel $n_i$. Thus,
		
		$r_i = s_i + n_i$,\quad$i=1,2$
		
		The transmitter sends $[0 \: 0]^T$ when "target is absent". In this problem, you have to tell the transmitter whether it should send $[1 \: -1]^T$ or $[1 \: 1]^T$ as the vector for "target present". The objective is to minimize the probability of error. The noise samples $n_1$ and $n_2$ are bi-variate Gaussian with zero mean and unit variance. Assume that $P(\text{Target Present}) = 0.5$.
		
		\begin{enumerate}
			\item[(a)] Which of the two vectors $[1 \: -1]^T$ or $[1 \: 1]^T$ should denote "target present" if the two noise samples are uncorrelated? Explain why?
			
			\item[(b)] Which of the two vectors $[1 \: -1]^T$ or $[1 \: 1]^T$ should denote "target present" if the two noise samples have a correlation coefficient of $\rho = 0.5$? Explain why?
		\end{enumerate}
	\end{enumerate}
\end{document}