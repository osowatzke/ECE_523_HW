\documentclass[fleqn]{article}
\usepackage[nodisplayskipstretch]{setspace}
\usepackage{amsmath, nccmath, bm}
\usepackage{amssymb}
\usepackage{enumitem}

\newcommand{\zerodisplayskip}{
	\setlength{\abovedisplayskip}{0pt}%
	\setlength{\belowdisplayskip}{0pt}%
	\setlength{\abovedisplayshortskip}{0pt}%
	\setlength{\belowdisplayshortskip}{0pt}%
	\setlength{\mathindent}{0pt}}
	
\newcommand{\norm}[1]{\left \lVert #1 \right \rVert}
	
\title{Homework 2}
\author{Owen Sowatzke}
\date{February 21, 2024}

\begin{document}

	\offinterlineskip
	\setlength{\lineskip}{12pt}
	\zerodisplayskip
	\maketitle
	
	\begin{enumerate}
		\item  The nearest neighbor method is used with the following labeled training data in a two class, two feature problem. Show clearly the decision boundaries obtained. Indicate all break points and slopes correctly
		
		Class 1 = $\{(0,0)^T,\:(0,1)^T\}$
		
		Class 2 = $\{(1,0)^T,\:(0,0.5)^T\}$
		
		Consider the distance between an input sample $\mathbf{x}$ and two training samples $\mathbf{x_1}$ and $\mathbf{x_2}$. By comparing the distances from the input sample to each training sample, we can determine which training sample the input is closest to. The boundary condition occurs when the input sample is equidistant from each training sample. 
		
		\begin{equation*}
			\norm{\mathbf{x} - \mathbf{x_2}} \overset{\omega_1}{\underset{\omega_2}{\gtrless}} \norm{\mathbf{x} - \mathbf{x_1}} \Rightarrow \norm{\mathbf{x} - \mathbf{x_2}}^2  \overset{\omega_1}{\underset{\omega_2}{\gtrless}} \norm{\mathbf{x} - \mathbf{x_1}}^2
		\end{equation*}
		
		\begin{equation*}
			(\mathbf{x} - \mathbf{x_2})^T(\mathbf{x} - \mathbf{x_2}) \overset{\omega_1}{\underset{\omega_2}{\gtrless}} (\mathbf{x} - \mathbf{x_1})^T(\mathbf{x} - \mathbf{x_1})
		\end{equation*}
		
		\begin{equation*}
			\norm{\mathbf{x}}^2 - \mathbf{x_2}^T\mathbf{x} - \mathbf{x}^T\mathbf{x_2} + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} \norm{\mathbf{x}}^2 - \mathbf{x_1}^T\mathbf{x} - \mathbf{x}^T\mathbf{x_1} + \norm{\mathbf{x_1}}^2
		\end{equation*}
		
		\begin{equation*}
			- \mathbf{x_2}^T\mathbf{x} - \mathbf{x}^T\mathbf{x_2} + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} - \mathbf{x_1}^T\mathbf{x} - \mathbf{x}^T\mathbf{x_1} + \norm{\mathbf{x_1}}^2
		\end{equation*}
		
		\begin{equation*}
			- (\mathbf{x_2}^T\mathbf{x})^T - \mathbf{x}^T\mathbf{x_2} + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} - (\mathbf{x_1}^T\mathbf{x})^T - \mathbf{x}^T\mathbf{x_1} + \norm{\mathbf{x_1}}^2
		\end{equation*}
		
		\begin{equation*}
			- \mathbf{x}^T\mathbf{x_2} - \mathbf{x}^T\mathbf{x_2} + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} - \mathbf{x}^T\mathbf{x_1} - \mathbf{x}^T\mathbf{x_1} + \norm{\mathbf{x_1}}^2
		\end{equation*}
		
		\begin{equation*}
			- 2\mathbf{x}^T\mathbf{x_2} + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} - 2\mathbf{x}^T\mathbf{x_1} + \norm{\mathbf{x_1}}^2
		\end{equation*}
		
		\begin{equation*}
			2\mathbf{x}^T\mathbf{x_1} - 2\mathbf{x}^T\mathbf{x_2} - \norm{\mathbf{x_1}}^2 + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0
		\end{equation*}
		
		\begin{equation*}
			2\mathbf{x}^T(\mathbf{x_1} - \mathbf{x_2}) - \norm{\mathbf{x_1}}^2 + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0
		\end{equation*}
		
		We start by finding the boundary conditions between the first sample of class 1 and the samples of class 2.
		
		Boundary between $(0,0)^T$ and $(1,0)^T$:
		
		\begin{equation*}
			2 \begin{bmatrix} x & y \end{bmatrix} \begin{bmatrix}-1\\ 0\end{bmatrix} - 0 + 1 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0 \Rightarrow -2x + 1 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0 \Rightarrow 2x \overset{\omega_2}{\underset{\omega_1}{\gtrless}} 1 \Rightarrow x \overset{\omega_2}{\underset{\omega_1}{\gtrless}} 0.5			
		\end{equation*}
		
		Boundary between $(0,0)^T$ and $(0,0.5)^T$:
		
		\begin{equation*}
			2 \begin{bmatrix} x & y \end{bmatrix} \begin{bmatrix}0\\ -0.5\end{bmatrix} - 0 + 0.25 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0 \Rightarrow -y + 0.25 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0 \Rightarrow y \overset{\omega_2}{\underset{\omega_1}{\gtrless}} 0.25		
		\end{equation*}
		
		$\therefore$ we are closer to sample $(0,0)^T$ if $y < 0.25$ and $x < 0.5$.
		
		Next, find the boundary conditions between the second sample of class 1 and the samples of class 2.
		
		Boundary between $(0,1)^T$ and $(1,0)^T$:
		
		\begin{equation*}
			2 \begin{bmatrix} x & y \end{bmatrix} \begin{bmatrix}-1\\ 1\end{bmatrix} - 1 + 1 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0 \Rightarrow -2x + 2y \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0 \Rightarrow 2y \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 2x \Rightarrow y \overset{\omega_1}{\underset{\omega_2}{\gtrless}} x		
		\end{equation*}
		
		Boundary between $(0,1)^T$ and $(0,0.5)^T$:
		
		\begin{equation*}
			2 \begin{bmatrix} x & y \end{bmatrix} \begin{bmatrix}0\\ 0.5\end{bmatrix} - 1 + 0.25 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0 \Rightarrow y - 0.75 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0 \Rightarrow y \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0.75		
		\end{equation*}
		
		$\therefore$ we are closer to sample $(0,1)^T$ if $y > 0.75$ and $y > x$.
		
		If we take the union between the two regions, we can determine the boundary conditions between class 1 and class 2.
		
		After taking the union of these regions, we find that we should choose class 1 if $y < 0.25$ and $x < 0.5$ or if $y > 0.75$ and $y > x$. 
		
		\item In a two class problem using two features, we have the following four training vectors:
		
		Class $\omega_1 = [(0,0)^T\quad(1,0)^T]$
		
		Class $\omega_2 = [(0,1)^T\quad(1,1)^T]$
		
		Use the perceptron criterion method with $\rho_k = 1$ to determine the solution vector $\mathbf{w}$. Use an all zero vector as the starting point for $\mathbf{w}$. Show the $\mathbf{w}$ vector after each iteration.
		
		\item Consider the following training data:
		
		Class $\omega_1 = [(0,0)^T\quad(1,0)^T\quad(0,-1)^T]$
		
		Class $\omega_2 = [(0,1)^T\quad(1,2)^T\quad(-1,0)^T]$
		
		Determine the MSE vector $\mathbf{w}$ if $\mathbf{b} = [1\:1\:1\:1\:1\:1]^T$. Use the generalized inverse to find the solution vector. Is the data linearly separable?
		
		For each sample of training data $\mathbf{x}$. Define the vector $\mathbf{\hat{x}}$ as follows:
		
		\begin{equation*}
			\mathbf{\hat{x}} = \begin{bmatrix} \mathbf{x} \\ 1 \end{bmatrix}			
		\end{equation*}
		
		For each sample $\mathbf{x}$ contained in class $\omega_2$, we need to multiply the vector $\mathbf{\hat{x}}$ by $-1$. After performing this multiplication, the definition for $\mathbf{\hat{x}}$  for samples in class $\omega_2$ is given by:
		
		\begin{equation*}
			\mathbf{\hat{x}} = \begin{bmatrix} -\mathbf{x} \\ -1 \end{bmatrix}			
		\end{equation*}
		
		We can then form the matrix $\mathbf{A}$ from the vectors $\mathbf{\hat{x}_i}$:
		
		\begin{equation*}
			\mathbf{A} = \begin{bmatrix} \mathbf{\hat{x}_1}^T \\ \vdots \\ \mathbf{\hat{x}_n}^T \end{bmatrix}					
		\end{equation*}
		
		The generalized inverse of $\mathbf{A}$ is then defined as:
		
		\begin{equation*}
			\mathbf{A}^{\dag} = (\mathbf{A^T}\mathbf{A})^{-1}\mathbf{A}^T
		\end{equation*}
		
		Using the generalized inverse $\mathbf{A}^{\dag}$, we can find the MSE vector $\mathbf{w}$.
		
		\begin{equation*}
			\mathbf{w} = \mathbf{A}^{\dag}\mathbf{b}
		\end{equation*}
			
		For the provided training data, we first need to solve for the matrix $\mathbf{A}$. Doing so results in the following:
		
		\begin{equation*}
			\mathbf{A} = \begin{bmatrix}
				0  &  0 &  1 \\
				1  &  0 &  1 \\
				0  & -1 &  1 \\
				0  & -1 & -1 \\
			   -1  & -2 & -1 \\
			    1  &  0 & -1
			\end{bmatrix}
		\end{equation*}
		
		Next, using the formula given above, we can solve for the generalized inverse $\mathbf{A}^{\dag}$, the result of this operation is given below:
		
		\begin{equation*}
			\mathbf{A}^{\dag} = \begin{bmatrix} 
			   -0.0270 &  0.4054 &  0.1081 &  0.1622 & -0.1351 &  0.4595 \\
			   -0.0541 & -0.1892 & -0.2838 & -0.1757 & -0.2703 & -0.0811 \\
			    0.1892 &  0.1622 &  0.2432 & -0.1351 & -0.0541 & -0.2162
			\end{bmatrix}
		\end{equation*}
		
		Finally, we can solve for the vector $\mathbf{w}$ as follows:
		
		\begin{equation*}
			\mathbf{w} = \mathbf{A}^{\dag}\mathbf{b} = \begin{bmatrix} 0.9730 \\ -1.0541 \\ 0.1892 \end{bmatrix}
		\end{equation*}
		
		The data is linearly separable if each element of $\mathbf{A}\mathbf{w}$ is greater than zero.
		
		\begin{equation*}
			\mathbf{A}\mathbf{w} = \begin{bmatrix}
				0.1892 \\
    				1.1622 \\
   	 			1.2432 \\
    				0.8649 \\
    				0.9459 \\
    				0.7838
			\end{bmatrix}
		\end{equation*}
		
		Because each element of $\mathbf{A}\mathbf{w}$ is greater than zero, the data is linearly separable.
		
	\end{enumerate}
\end{document}