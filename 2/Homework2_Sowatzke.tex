\documentclass[fleqn]{article}
\usepackage[nodisplayskipstretch]{setspace}
\usepackage{amsmath, nccmath, bm}
\usepackage{amssymb}
\usepackage{enumitem}

\newcommand{\zerodisplayskip}{
	\setlength{\abovedisplayskip}{0pt}%
	\setlength{\belowdisplayskip}{0pt}%
	\setlength{\abovedisplayshortskip}{0pt}%
	\setlength{\belowdisplayshortskip}{0pt}%
	\setlength{\mathindent}{0pt}}
	
\newcommand{\norm}[1]{\left \lVert #1 \right \rVert}
	
\title{Homework 2}
\author{Owen Sowatzke}
\date{February 21, 2024}

\begin{document}

	\offinterlineskip
	\setlength{\lineskip}{12pt}
	\zerodisplayskip
	\maketitle
	
	\begin{enumerate}
		\item  The nearest neighbor method is used with the following labeled training data in a two class, two feature problem. Show clearly the decision boundaries obtained. Indicate all break points and slopes correctly
		
		Class 1 = $\{(0,0)^T,\:(0,1)^T\}$
		
		Class 2 = $\{(1,0)^T,\:(0,0.5)^T\}$
		
		Consider the distance between a random input sample $\mathbf{x}$ and a training sample of either Class 1 or Class 2, which we will define as $\mathbf{x_1}$ and $\mathbf{x_2}$ respectively. In the nearest neighbor method, we will classify the input sample based on which of the training samples it is closest to. The threshold, then, occurs when the input sample is equidistant from both samples of training data.
		
		\begin{equation*}
			\norm{\mathbf{x} - \mathbf{x_2}} \overset{\omega_1}{\underset{\omega_2}{\gtrless}} \norm{\mathbf{x} - \mathbf{x_1}} \Rightarrow \norm{\mathbf{x} - \mathbf{x_2}}^2  \overset{\omega_1}{\underset{\omega_2}{\gtrless}} \norm{\mathbf{x} - \mathbf{x_1}}^2
		\end{equation*}
		
		\begin{equation*}
			(\mathbf{x} - \mathbf{x_2})^T(\mathbf{x} - \mathbf{x_2}) \overset{\omega_1}{\underset{\omega_2}{\gtrless}} (\mathbf{x} - \mathbf{x_1})^T(\mathbf{x} - \mathbf{x_1})
		\end{equation*}
		
		\begin{equation*}
			\norm{\mathbf{x}}^2 - \mathbf{x_2}^T\mathbf{x} - \mathbf{x}^T\mathbf{x_2} + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} \norm{\mathbf{x}}^2 - \mathbf{x_1}^T\mathbf{x} - \mathbf{x}^T\mathbf{x_1} + \norm{\mathbf{x_1}}^2
		\end{equation*}
		
		\begin{equation*}
			- \mathbf{x_2}^T\mathbf{x} - \mathbf{x}^T\mathbf{x_2} + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} - \mathbf{x_1}^T\mathbf{x} - \mathbf{x}^T\mathbf{x_1} + \norm{\mathbf{x_1}}^2
		\end{equation*}
		
		\begin{equation*}
			- (\mathbf{x_2}^T\mathbf{x})^T - \mathbf{x}^T\mathbf{x_2} + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} - (\mathbf{x_1}^T\mathbf{x})^T - \mathbf{x}^T\mathbf{x_1} + \norm{\mathbf{x_1}}^2
		\end{equation*}
		
		\begin{equation*}
			- \mathbf{x}^T\mathbf{x_2} - \mathbf{x}^T\mathbf{x_2} + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} - \mathbf{x}^T\mathbf{x_1} - \mathbf{x}^T\mathbf{x_1} + \norm{\mathbf{x_1}}^2
		\end{equation*}
		
		\begin{equation*}
			- 2\mathbf{x}^T\mathbf{x_2} + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} - 2\mathbf{x}^T\mathbf{x_1} + \norm{\mathbf{x_1}}^2
		\end{equation*}
		
		\begin{equation*}
			2\mathbf{x}^T\mathbf{x_1} - 2\mathbf{x}^T\mathbf{x_2} - \norm{\mathbf{x_1}}^2 + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0
		\end{equation*}
		
		\begin{equation*}
			2\mathbf{x}^T(\mathbf{x_1} - \mathbf{x_2}) - \norm{\mathbf{x_1}}^2 + \norm{\mathbf{x_2}}^2 \overset{\omega_1}{\underset{\omega_2}{\gtrless}} 0
		\end{equation*}
		
		\item In a two class problem using two features, we have the following four training vectors:
		
		Class $\omega_1 = [(0,0)^T\quad(1,0)^T]$
		
		Class $\omega_2 = [(0,1)^T\quad(1,1)^T]$
		
		Use the perceptron criterion method with $\rho_k = 1$ to determine the solution vector $\mathbf{w}$. Use an all zero vector as the starting point for $\mathbf{w}$. Show the $\mathbf{w}$ vector after each iteration.
		
		\item Consider the following training data:
		
		Class $\omega_1 = [(0,0)^T\quad(1,0)^T\quad(0,-1)^T]$
		
		Class $\omega_2 = [(0,1)^T\quad(1,2)^T\quad(-1,0)^T]$
		
		Determine the MSE vector $\mathbf{w}$ if $\mathbf{b} = [1\:1\:1\:1\:1\:1]^T$. Use the generalized inverse to find the solution vector. Is the data linearly separable?
		
		For each sample of training data $\mathbf{x}$. Define the vector $\mathbf{\hat{x}}$ as follows:
		
		\begin{equation*}
			\mathbf{\hat{x}} = \begin{bmatrix} \mathbf{x} \\ 1 \end{bmatrix}			
		\end{equation*}
		
		For each sample $\mathbf{x}$ contained in class $\omega_2$, we need to multiply the vector $\mathbf{\hat{x}}$ by $-1$. After performing this multiplication, the definition for $\mathbf{\hat{x}}$  for samples in class $\omega_2$ is given by:
		
		\begin{equation*}
			\mathbf{\hat{x}} = \begin{bmatrix} -\mathbf{x} \\ -1 \end{bmatrix}			
		\end{equation*}
		
		We can concatenate the vectors $\mathbf{\hat{x}}$ to form a matrix $\mathbf{A}$.
		
		\begin{equation*}
			\mathbf{A} = \begin{bmatrix} \mathbf{\hat{x}_1} & \cdots & \mathbf{\hat{x}_n}\end{bmatrix}					
		\end{equation*}
		
		The generalized inverse of $\mathbf{A}$ is then defined as:
		
		\begin{equation*}
			\mathbf{A}^{\dag} = (\mathbf{A}\mathbf{A}^T)^{-1}\mathbf{A}
		\end{equation*}
		
		Using the generalized inverse $\mathbf{A}^{\dag}$, we can find the MSE vector $\mathbf{w}$.
		
		\begin{equation*}
			\mathbf{w} = \mathbf{A}^{\dag}\mathbf{b}
		\end{equation*}
	\end{enumerate}
\end{document}